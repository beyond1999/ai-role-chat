# AI Role Chat (MVP)

### æ¼”ç¤ºè§†é¢‘åœ°å€ï¼š
https://www.bilibili.com/video/BV1i1nZz8Ert/?spm_id_from=333.1387.homepage.video_card.click&vd_source=a2a1ad78786d36c0c9607eb4bfe43853

**è¯­éŸ³ â†’ ASR â†’ LLM â†’ TTS çš„å®æ—¶æ•°å­—äººæœ€å°å¯ç”¨ç³»ç»Ÿ**

- å‰ç«¯ï¼šå•é¡µ HTMLï¼ˆé€‰æ‹©äººè®¾ï¼š**å­™æ‚Ÿç©º / å“ˆåˆ©Â·æ³¢ç‰¹ / é’¢é“ä¾ **ï¼Œå¸¦å¤´åƒä¸å¼€åœºç™½ï¼‰
- ASRï¼š`FastAPI + WebSocket + faster-whisper`ï¼ˆç«¯ç‚¹æ£€æµ‹ã€å»é‡ã€ä½å»¶è¿Ÿï¼‰
- LLMï¼š`FastAPI` ä»£ç†åˆ° **llama.cpp/llama-server**ï¼ˆQwen2.5 GGUFï¼‰ï¼ŒOpenAI å…¼å®¹æˆ– `/completion` å›é€€
- TTSï¼šæµè§ˆå™¨ `SpeechSynthesis`ï¼ˆå ä½ï¼›å¯æ›¿æ¢ä¸º `edge-tts` æœåŠ¡ï¼‰

---

## ç›®å½•
- [æ¶æ„](#æ¶æ„)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [å‰ç«¯](#å‰ç«¯)
- [ASR åç«¯](#asr-åç«¯)
- [LLM æ¨¡å—](#llm-æ¨¡å—)
- [æ¶ˆæ¯ä¸åè®®](#æ¶ˆæ¯ä¸åè®®)
- [é…ç½®é¡¹](#é…ç½®é¡¹)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [Roadmap](#roadmap)

---

## æ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   PCM(16k/Int16)   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   partial/final   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å‰ç«¯é¡µé¢ â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  ASR(WebSocket)â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  å‰ç«¯é¡µé¢  â”‚
â”‚  (éº¦å…‹é£) â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  faster-whisper â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  (å±•ç¤º)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚  é€‰äººè®¾+æ–‡æœ¬  POST /llm                                          â”‚ TTS     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚         â”‚
                                                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚ LLM æ¨¡å—(FastAPI)       â”‚  
     å‰ç«¯ POST /llm â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ ä»£ç† llama-server       â”‚â”€â”€â–¶ llama.cpp `llama-server`
                                    â”‚ (OpenAI /completion)    â”‚   (Qwen2.5 GGUF)
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
> è¯´æ˜ï¼šç›®å‰ç”±**å‰ç«¯**åœ¨æ”¶åˆ° ASR çš„ `final` åè°ƒç”¨ `/llm`ï¼ŒASR ç«¯**ä¸å†**è½¬å‘åˆ° LLMï¼ˆé¿å…é‡å¤ï¼‰ã€‚

---

## å¿«é€Ÿå¼€å§‹

### 0) ä¾èµ–
- Python 3.10+
- å·²ç¼–è¯‘å¥½çš„ `llama.cpp/llama-server`ï¼ˆæˆ–ä½¿ç”¨ä½ ç°æœ‰çš„äºŒè¿›åˆ¶ï¼‰
- æµè§ˆå™¨ï¼ˆæ¨è Chrome/Edgeï¼‰

```bash
pip install fastapi "uvicorn[standard]" httpx pydantic numpy aiohttp faster-whisper websockets
```

### 1) å¯åŠ¨æœ¬åœ° LLMï¼ˆQwen2.5 GGUFï¼‰
ä½¿ç”¨ä½ ç°æœ‰è„šæœ¬ï¼ˆç¤ºä¾‹ï¼‰ï¼š
```bash
#!/usr/bin/env bash
set -e
MODEL=~/models/qwen2.5/3b-instruct/Qwen2.5-3B-Instruct-Q4_K_M.gguf
PORT=${PORT:-8080}
HOST=${HOST:-0.0.0.0}
CTX=${CTX:-4096}
TEMP=${TEMP:-0.7}
RP=${RP:-1.1}
GPU_LAYERS=${GPU_LAYERS:-35}
cd ~/work/llama.cpp/build/bin
./llama-server -m "$MODEL" --host "$HOST" --port "$PORT" \
  -c "$CTX" --temp "$TEMP" --repeat-penalty "$RP" --n-gpu-layers "$GPU_LAYERS"
```
> ç«¯å£ç¼ºçœï¼š`8080`ã€‚

### 2) å¯åŠ¨ LLM æ¨¡å—ï¼ˆFastAPI ä»£ç†ï¼‰
`backend/llm/llm_app.py`ï¼ˆå·²é€‚é… llama.cpp OpenAI å…¼å®¹ä¸ `/completion` å›é€€ï¼‰ï¼š
```bash
export LLAMA_BASE=http://127.0.0.1:8080
python -m uvicorn backend.llm.llm_app:app --host 127.0.0.1 --port 8001 --workers 1 --reload false
```
> å¥åº·æ£€æŸ¥ï¼š`curl http://127.0.0.1:8001/health`ã€‚

### 3) å¯åŠ¨ ASR æœåŠ¡
`backend/asr/asr_app.py`ï¼š
```bash
# æ³¨æ„ï¼šå…³é—­ reloadã€å¤š workerï¼Œé¿å…é‡å¤å¤„ç†
python -m uvicorn backend.asr.asr_app:app --host 0.0.0.0 --port 8000 --workers 1 --reload false
```
> **WebSocket è·¯å¾„**ï¼šä½ çš„ä»£ç æ˜¯ `@app.websocket("/ws")`ã€‚è‹¥å‰ç«¯å¸¸é‡æ˜¯ `/ws_asr`ï¼Œè¯·æ”¹æˆ `/ws` æˆ–åŒæ­¥äºŒè€…ã€‚

### 4) æ‰“å¼€å‰ç«¯
å»ºè®®ä½¿ç”¨æœ¬åœ°é™æ€æœåŠ¡ï¼ˆéº¦å…‹é£æƒé™åœ¨ `http://localhost` ä¸‹æ›´ç¨³å®šï¼‰ï¼š
```bash
# åœ¨å‰ç«¯ HTML æ‰€åœ¨ç›®å½•
python -m http.server 5173
# æµè§ˆå™¨è®¿é—® http://127.0.0.1:5173
```
åœ¨é¡µé¢é‡Œï¼šé€‰æ‹©**äººè®¾** â†’ ç‚¹éº¦å…‹é£è®²è¯ â†’ å‡ºç° `partial`/`final` â†’ è‡ªåŠ¨å‘ç»™ LLM â†’ æµè§ˆå™¨æœ—è¯»å›å¤ã€‚

---

## å‰ç«¯
æ–‡ä»¶ï¼š`AI Role Chat MVP (ä¸‰äººæ ¼+å¤´åƒ+å¼€åœºå¯¹ç™½).html`

- **äººè®¾**ï¼š
  - å­™æ‚Ÿç©ºï¼ˆğŸµï¼‰  
  - å“ˆåˆ©Â·æ³¢ç‰¹ï¼ˆğŸ§™â€â™‚ï¸ï¼‰  
  - é’¢é“ä¾ ï¼ˆğŸ¤–ï¼‰
- åˆ‡æ¢äººè®¾ä¼šæ¸…ç©ºä¼šè¯å¹¶å‘é€**å¼€åœºç™½**ï¼›`localStorage` è®°ä½ä¸Šæ¬¡é€‰æ‹©ã€‚
- **å¸¸é‡**ï¼š
  ```js
  const LLM_URL  = "http://127.0.0.1:8001/llm";
  const ASR_WS_URL = "ws://127.0.0.1:8000/ws_asr"; 
  ```
- **éŸ³é¢‘**ï¼š`AudioWorklet` é‡‡é›†æµ®ç‚¹ PCM â†’ **å‰ç«¯é‡é‡‡æ ·è‡³ 16kHz** â†’ è½¬ Int16 â†’ 20ms ä¸€åŒ…é€šè¿‡ WS å‘é€ã€‚
- **TTS**ï¼šé»˜è®¤ç”¨æµè§ˆå™¨ `speechSynthesis`ï¼ˆå¯æ›¿æ¢åç«¯ TTSï¼‰ã€‚

---

## ASR åç«¯
- æ¡†æ¶ï¼š`FastAPI` + `WebSocket`
- æ¨¡å‹ï¼š`faster-whisper`ï¼ˆCTranslate2ï¼‰ï¼Œé»˜è®¤ `small`ï¼ˆå¯æ¢ `medium/large-v2`ï¼‰
- æ ¸å¿ƒï¼š
  - **ç¯å½¢ç¼“å†²**ï¼šæ¥æ”¶è¿ç»­ PCM16ï¼›
  - **ç«¯ç‚¹å™¨**ï¼šRMS é™éŸ³åˆ¤å®š + ä¸‰è§„åˆ™ï¼ˆæ ‡ç‚¹çŸ­åœ/é•¿é™éŸ³/æ–‡æœ¬ç¨³å®šï¼‰ï¼›
  - **å»é‡**ï¼š
    - åªåœ¨ `partial` å˜åŒ–æ—¶ä¸‹å‘ï¼›
    - `final` åæ¨è¿› **æ¶ˆè´¹æ¸¸æ ‡** `last_final_offset`ï¼Œä¸‹ä¸€è½®ä»…è§£ç æœªæ¶ˆè´¹éŸ³é¢‘ï¼›
    - ç”¨ `last_final_text` åšä¸€æ¬¡å¹‚ç­‰å…œåº•ã€‚
- é‡è¦å‚æ•°ï¼š
  - `TICK_SECONDS=0.25`ã€`DECODE_WINDOW_SECONDS=8`
  - `END_SILENCE_MS=800`ã€`SHORT_PAUSE_MS=300`ã€`STABLE_NOCHANGE_MS=1500`
  - `SILENCE_RMS_THRESHâ‰ˆ0.005`ï¼ˆæŒ‰éº¦å…‹é£åº•å™ªå¾®è°ƒï¼‰
  - `LANGUAGE="zh"`ï¼ˆé¿å…è‡ªåŠ¨åˆ¤æˆè‹±æ–‡å¯¼è‡´â€œthank youâ€ï¼‰

> **ä¸è¦**åŒæ—¶ç”± ASR ä¸å‰ç«¯éƒ½æŠŠ `final` å‘ç»™ LLMã€‚å½“å‰ç‰ˆæœ¬å·²é‡‡ç”¨**å‰ç«¯å‘é€**ï¼Œè¯·ç¡®ä¿ ASR é‡Œæ³¨é‡Šæ‰ `push_to_webhook(final_text)`ã€‚

---

## LLM æ¨¡å—
- æ¡†æ¶ï¼š`FastAPI`ï¼Œç«¯å£ `8001`
- è¡Œä¸ºï¼šå°†å‰ç«¯ `messages` é€ä¼ è‡³ llama.cppï¼š
  - ä¼˜å…ˆ `POST /v1/chat/completions`ï¼ˆOpenAIå…¼å®¹ï¼‰
  - è‹¥ 404ï¼Œå›é€€ `POST /completion`ï¼ˆå°† messages æ‹¼æˆ promptï¼‰
- å¹‚ç­‰ï¼šå¯ä¼  `X-Idempotency-Key`ï¼Œæ¨¡å—å†…å­˜ç¼“å­˜ 10 åˆ†é’Ÿï¼Œé‡å¤é”®å¤ç”¨é¦–ä¸ªç»“æœã€‚
- ç¯å¢ƒå˜é‡ï¼š
  - `LLAMA_BASE`ï¼ˆé»˜è®¤ `http://127.0.0.1:8080`ï¼‰
  - `LLAMA_MODEL`ï¼ˆæ ‡è¯†ç”¨é€”ï¼‰
  - `LLAMA_TIMEOUT`ï¼ˆé»˜è®¤ 30sï¼‰

**ç¤ºä¾‹è¯·æ±‚**
```bash
curl -s http://127.0.0.1:8001/llm \
 -H 'Content-Type: application/json' \
 -H 'X-Idempotency-Key: demo-1' \
 -d '{
  "messages":[
    {"role":"system","content":"ä½ æ˜¯æ‰˜å°¼Â·æ–¯å¡”å…‹é£æ ¼ï¼šå…ˆç»“è®ºåç»†èŠ‚ã€‚"},
    {"role":"user","content":"ç»™æˆ‘ä¸¤æ¡è‹å·ä¸€æ—¥æ¸¸å»ºè®®"}
  ],
  "temperature":0.7,
  "max_tokens":256
 }' | jq .
```

---

## æ¶ˆæ¯ä¸åè®®

### WebSocketï¼ˆASRï¼‰
- **è¿æ¥**ï¼š`ws://<host>:8000/ws`
- **å®¢æˆ·ç«¯ â†’ æœåŠ¡ç«¯**ï¼š
  1. å¯é€‰æ–‡æœ¬é…ç½®ï¼š`{"op":"config","sampleRate":16000}`
  2. äºŒè¿›åˆ¶å¸§ï¼š**PCM16**ï¼ˆå•å£°é“ï¼Œ16kHzï¼‰ï¼Œå»ºè®® **20ms/å¸§**
- **æœåŠ¡ç«¯ â†’ å®¢æˆ·ç«¯**ï¼š
  - `{"type":"partial","text":"...","language":"zh"}`
  - `{"type":"final","text":"...","segments":[{"start":0.0,"end":1.2,"text":"..."}]}`

### HTTPï¼ˆLLMï¼‰
- `POST /llm`
  - Bodyï¼š`{ messages:[{role,content}...], temperature?, max_tokens?, session_id? }`
  - Headerï¼ˆå¯é€‰ï¼‰ï¼š`X-Idempotency-Key: <session:utter_idx:hash>`
  - è¿”å›ï¼š`{"text":"...","model":"...","cached":false}`

---

## é…ç½®é¡¹

### ASR å…³é”®é…ç½®ï¼ˆ`asr_app.py`ï¼‰
- `MODEL_PATH`ï¼šfaster-whisper æ¨¡å‹ç›®å½•ï¼ˆæˆ–ç”¨åå­—è®©å…¶è‡ªåŠ¨ä¸‹è½½ï¼‰
- `WHISPER_DEVICE`ï¼š`cuda` 12.9 cudnn 12.9/`cpu`
- `WHISPER_COMPUTE_TYPE`ï¼š`float16`/`int8`/`int8_float16`
- `LANGUAGE`ï¼šå»ºè®®å›ºå®šä¸º `"zh"`
- ç«¯ç‚¹æ£€æµ‹ç›¸å…³ï¼š`END_SILENCE_MS`ã€`SILENCE_RMS_THRESH` ç­‰

### LLM æ¨¡å—ï¼ˆ`llm_app.py`ï¼‰
- `LLAMA_BASE`ã€`LLAMA_TIMEOUT`ã€`LLAMA_MODEL`

### å‰ç«¯ï¼ˆHTMLï¼‰
- `LLM_URL`ã€`ASR_WS_URL`

